{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process events files and generates trials for sessions prior to 20240416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import session_processing_helper as helper\n",
    "import utils\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rebekahzhang/data/behavior_data/pre_meta_change/exp2/cohort_5\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/rebekahzhang/data/behavior_data'\n",
    "period = 'pre_meta_change'\n",
    "exp = \"exp2\"\n",
    "cohort = \"cohort_5\"\n",
    "data_folder = os.path.join(data_dir, period, exp, cohort)\n",
    "print(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check session folders have both meta and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_session_files(data_folder):\n",
    "    files_check = []\n",
    "    for entry in os.scandir(data_folder):\n",
    "        if entry.is_dir():\n",
    "            dir = entry.name\n",
    "            session_path = os.path.join(data_folder, dir)\n",
    "            events_found = False\n",
    "            meta_found = False\n",
    "            events_empty = True\n",
    "            meta_empty = True\n",
    "            \n",
    "            required_files = [f for f in os.scandir(session_path) if f.is_file() and not f.name.startswith('.')]\n",
    "            \n",
    "            for file in required_files:\n",
    "                if file.name.startswith(\"events_\"):\n",
    "                    events_found = True\n",
    "                    if file.stat().st_size > 0:\n",
    "                        events_empty = False\n",
    "                elif file.name.startswith(\"meta_\"):\n",
    "                    meta_found = True\n",
    "                    if file.stat().st_size > 0:\n",
    "                        meta_empty = False\n",
    "            \n",
    "            files_check.append({\n",
    "                'dir': dir,\n",
    "                'events': events_found,\n",
    "                'meta': meta_found,\n",
    "                'events_empty': events_empty if events_found else None,\n",
    "                'meta_empty': meta_empty if meta_found else None\n",
    "            })\n",
    "\n",
    "    files_check_df = pd.DataFrame(files_check).sort_values(\"dir\")\n",
    "    missing_meta = files_check_df[files_check_df.meta==False]\n",
    "    missing_events = files_check_df[files_check_df.events==False]\n",
    "    empty_meta = files_check_df[(files_check_df.meta==True) & (files_check_df.meta_empty==True)]\n",
    "    empty_events = files_check_df[(files_check_df.events==True) & (files_check_df.events_empty==True)]\n",
    "    \n",
    "    return missing_meta, missing_events, empty_meta, empty_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All sessions have non-empty meta and events files.\n"
     ]
    }
   ],
   "source": [
    "missing_events, missing_meta, empty_meta, empty_events = helper.check_session_files(data_folder)\n",
    "if not (missing_meta.empty and missing_events.empty and empty_meta.empty and empty_events.empty):\n",
    "    print(\"\\nFile check results:\")\n",
    "    if not missing_meta.empty:\n",
    "        print(\"\\nSessions missing meta files:\")\n",
    "        display(missing_meta)\n",
    "    if not missing_events.empty:\n",
    "        print(\"\\nSessions missing events files:\")\n",
    "        display(missing_events)\n",
    "    if not empty_meta.empty:\n",
    "        print(\"\\nSessions with empty meta files:\")\n",
    "        display(empty_meta)\n",
    "    if not empty_events.empty:\n",
    "        print(\"\\nSessions with empty events files:\")\n",
    "        display(empty_events)\n",
    "else:\n",
    "    print(\"\\nAll sessions have non-empty meta and events files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.delete_folders(missing_meta.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.delete_folders(missing_events.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.delete_folders(empty_meta.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.delete_folders(empty_events.dir.tolist(), data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save sessions log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate session log using meta data from each session and add columns of basic info to each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_sessions_all(sessions_all):\n",
    "    sessions_all['dir'] = sessions_all['date']+ '_' + sessions_all['time'] + '_' + sessions_all['mouse']\n",
    "    sessions_all = sessions_all.sort_values('dir')\n",
    "    sessions_all[['exp', 'group']] = sessions_all['exp'].str.extract(r'exp(\\d)_(short|long)')\n",
    "    sessions_all['group'] = sessions_all['group'].map({'short': 's', 'long': 'l'})\n",
    "    return sessions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sessions_all(data_folder):\n",
    "    \"\"\"Generates a DataFrame using session metadata from JSON files.\"\"\"\n",
    "\n",
    "    data = []\n",
    "    for root, _, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"meta_\") and file.endswith(\".json\"):\n",
    "                path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(path) as f:\n",
    "                        session_data = json.load(f)\n",
    "                        data.append(session_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    sessions_all = pd.DataFrame(data)\n",
    "    sessions_all = modify_sessions_all(sessions_all)\n",
    "    return sessions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sessions_training(sessions_all):\n",
    "    sessions_training = sessions_all.loc[sessions_all.training == 'regular'].reset_index()\n",
    "    sessions_training = sessions_training.groupby('mouse', group_keys=False).apply(helper.assign_session_numbers)\n",
    "    return sessions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_logs(data_folder, save_logs=True):\n",
    "    sessions_all = generate_sessions_all(data_folder)\n",
    "    sessions_training = generate_sessions_training(sessions_all)\n",
    "    if save_logs:\n",
    "        utils.save_as_csv(df=sessions_all, folder=data_folder, filename='sessions_all.csv')\n",
    "        utils.save_as_csv(df=sessions_training, folder=data_folder, filename='sessions_training.csv')\n",
    "    print(f\"{len(sessions_training)} sessions in total\")\n",
    "    return sessions_all, sessions_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-run after every quality control steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 sessions in total\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mouse</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>exp</th>\n",
       "      <th>training</th>\n",
       "      <th>rig</th>\n",
       "      <th>pump_ul_per_turn</th>\n",
       "      <th>total_trial</th>\n",
       "      <th>total_reward</th>\n",
       "      <th>avg_tw</th>\n",
       "      <th>dir</th>\n",
       "      <th>group</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>175</td>\n",
       "      <td>RZ034</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>12-42-19</td>\n",
       "      <td>2</td>\n",
       "      <td>regular</td>\n",
       "      <td>rig2</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>356</td>\n",
       "      <td>700</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2024-04-15_12-42-19_RZ034</td>\n",
       "      <td>s</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>58</td>\n",
       "      <td>RZ036</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>12-43-20</td>\n",
       "      <td>2</td>\n",
       "      <td>regular</td>\n",
       "      <td>rig3</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>524</td>\n",
       "      <td>700</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2024-04-15_12-43-20_RZ036</td>\n",
       "      <td>s</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>74</td>\n",
       "      <td>RZ037</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>13-23-53</td>\n",
       "      <td>2</td>\n",
       "      <td>regular</td>\n",
       "      <td>rig2</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>270</td>\n",
       "      <td>700</td>\n",
       "      <td>5.86</td>\n",
       "      <td>2024-04-15_13-23-53_RZ037</td>\n",
       "      <td>l</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>116</td>\n",
       "      <td>RZ038</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>13-39-14</td>\n",
       "      <td>2</td>\n",
       "      <td>regular</td>\n",
       "      <td>rig3</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>129</td>\n",
       "      <td>285</td>\n",
       "      <td>6.03</td>\n",
       "      <td>2024-04-15_13-39-14_RZ038</td>\n",
       "      <td>l</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>295</td>\n",
       "      <td>RZ039</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>14-48-29</td>\n",
       "      <td>2</td>\n",
       "      <td>regular</td>\n",
       "      <td>rig2</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>466</td>\n",
       "      <td>700</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2024-04-15_14-48-29_RZ039</td>\n",
       "      <td>l</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  mouse        date      time exp training   rig  pump_ul_per_turn  \\\n",
       "280    175  RZ034  2024-04-15  12-42-19   2  regular  rig2            0.0686   \n",
       "281     58  RZ036  2024-04-15  12-43-20   2  regular  rig3            0.0659   \n",
       "282     74  RZ037  2024-04-15  13-23-53   2  regular  rig2            0.0686   \n",
       "283    116  RZ038  2024-04-15  13-39-14   2  regular  rig3            0.0659   \n",
       "284    295  RZ039  2024-04-15  14-48-29   2  regular  rig2            0.0686   \n",
       "\n",
       "     total_trial  total_reward  avg_tw                        dir group  \\\n",
       "280          356           700    2.12  2024-04-15_12-42-19_RZ034     s   \n",
       "281          524           700    1.27  2024-04-15_12-43-20_RZ036     s   \n",
       "282          270           700    5.86  2024-04-15_13-23-53_RZ037     l   \n",
       "283          129           285    6.03  2024-04-15_13-39-14_RZ038     l   \n",
       "284          466           700    1.79  2024-04-15_14-48-29_RZ039     l   \n",
       "\n",
       "     session  \n",
       "280       56  \n",
       "281       57  \n",
       "282       56  \n",
       "283       55  \n",
       "284       56  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions_all, sessions_training = generate_session_logs(data_folder)\n",
    "sessions_training.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted sessions\n",
    "doesn't need to run when data folder is cleaned \n",
    "<br>\n",
    "sessions_all needs to be regenerated after every cleaning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove crashed sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all sessions are perfect! woohoo!\n"
     ]
    }
   ],
   "source": [
    "sessions_crashed = pd.DataFrame(columns=sessions_training.columns)\n",
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        events_path = utils.generate_events_path(data_folder, session_info)\n",
    "        events = pd.read_csv(events_path, low_memory=False)\n",
    "        session_end = events.loc[(events.key=='session') & (events.value==0)]\n",
    "\n",
    "        if not len(session_end)==1:\n",
    "            sessions_crashed = pd.concat([sessions_crashed, session_info.to_frame().T], ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "\n",
    "if len(problematic_sessions) > 0:\n",
    "    print(\"cannot open: \")\n",
    "    display(problematic_sessions)\n",
    "\n",
    "if len(sessions_crashed) > 0:\n",
    "    print(\"crashed sessions: \")\n",
    "    display(sessions_crashed)\n",
    "\n",
    "if len(problematic_sessions) == 0 and len(sessions_crashed) == 0:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.remove_sessions(sessions_crashed, data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for short sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no short sessions to be checked!\n"
     ]
    }
   ],
   "source": [
    "short_threshold = 20\n",
    "sessions_short = sessions_all[(sessions_all['total_trial'] < short_threshold) | sessions_all['total_trial'].isna()]\n",
    "if len(sessions_short)>0:\n",
    "    display(sessions_short)\n",
    "else: \n",
    "    print('no short sessions to be checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove short sessions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no sessions to delete\n"
     ]
    }
   ],
   "source": [
    "utils.remove_sessions(sessions_short, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load session log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = utils.load_data(os.path.join(data_folder, 'sessions_training.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needs to align trial number, add trial time, before trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_trial_num(events):\n",
    "    max_trial_num = events['session_trial_num'].max()\n",
    "    last_trial = events.loc[events['session_trial_num'] == max_trial_num]\n",
    "    session_end = last_trial.loc[(last_trial['key'] == 'session') & (last_trial['value'] == 0)]\n",
    "    last_trial_end = last_trial.loc[(last_trial['key'] == 'trial') & (last_trial['value'] == 0)]\n",
    "    if len(session_end) > 0 and len(last_trial_end) > 0:\n",
    "        return max_trial_num\n",
    "    else:\n",
    "        return max_trial_num - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw session data\n",
    "def generate_trials(events, max_trial_num):\n",
    "    trial_info_list = []\n",
    "    for t in range(int(max_trial_num)+1):\n",
    "        trial = events.loc[events['session_trial_num'] == t]\n",
    "        trial_basics = helper.get_trial_basics(trial)\n",
    "        trial_info_list.append(trial_basics)\n",
    "    trials = pd.DataFrame(trial_info_list)\n",
    "    return trials\n",
    "\n",
    "def align_trial_number(session, trials):\n",
    "    for _, trial_basics in trials.iterrows():\n",
    "        session.loc[session['session_time'].between(trial_basics['start_time'], trial_basics['end_time']), \n",
    "                'block_num'] = trial_basics['block_num']\n",
    "        session.loc[session['session_time'].between(trial_basics['start_time'], trial_basics['end_time']), \n",
    "                'session_trial_num'] = trial_basics['session_trial_num']\n",
    "        session.loc[session['session_time'].between(trial_basics['start_time'], trial_basics['end_time']), \n",
    "                'block_trial_num'] = trial_basics['block_trial_num']\n",
    "    return session\n",
    "\n",
    "def align_trial_states(trial):\n",
    "    bg_start_time = trial.loc[(trial['key'] == 'background') & (trial['value'] == 1)].iloc[0]['session_time']\n",
    "    wait_start_time = trial.loc[(trial['key'] == 'wait') & (trial['value'] == 1)].iloc[0]['session_time']\n",
    "    if 'consumption' in trial.key.unique():\n",
    "        consumption_start_time = trial.loc[(trial['key'] == 'consumption') & (trial['value'] == 1)].iloc[0]['session_time']\n",
    "    else:\n",
    "        consumption_start_time = math.nan\n",
    "    \n",
    "    trial.loc[(trial.session_time > bg_start_time) & (trial.session_time < wait_start_time), 'state'] = 'in_background'\n",
    "    trial.loc[(trial.session_time > wait_start_time) & (trial.session_time < consumption_start_time), 'state'] = 'in_wait'\n",
    "    trial.loc[trial.session_time > consumption_start_time, 'state'] = 'in_consumption'\n",
    "    return trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        events_processed_path = utils.generate_events_processed_path(data_folder, session_info)\n",
    "        # if os.path.isfile(events_processed_path):\n",
    "        #     continue\n",
    "        events = pd.read_csv(utils.generate_events_path(data_folder, session_info), low_memory=False)\n",
    "        max_trial_num = get_max_trial_num(events)\n",
    "\n",
    "        trials = generate_trials(events, max_trial_num)\n",
    "        events = align_trial_number(events, trials)\n",
    "        events = events.loc[events['session_trial_num'].between(0, max_trial_num)]\n",
    "\n",
    "        events = events.groupby('session_trial_num', group_keys=False).apply(align_trial_states)\n",
    "        # add trial_time\n",
    "        events_processed = events.groupby('session_trial_num', group_keys=False).apply(helper.add_trial_time)\n",
    "        events_processed.to_csv(events_processed_path)\n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "        \n",
    "if len(problematic_sessions) > 0:\n",
    "    display(problematic_sessions)\n",
    "else:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set curation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated rounds dict cuz we no longer run more than 1 session per day\n",
    "mouse_list = utils.generate_mouse_list(sessions_all)\n",
    "sessions_by_date = sessions_training.groupby('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_missing_sessions = True\n",
    "for date, data in sessions_by_date:\n",
    "    for mouse in mouse_list:\n",
    "        mouse_by_date = data.loc[data['mouse'] == mouse]\n",
    "        if len(mouse_by_date) < 1:\n",
    "            no_missing_sessions = False\n",
    "            print(f\"on {date}, {mouse} has missing sessions\")\n",
    "if no_missing_sessions:\n",
    "    print(\"no missing sessions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessions_by_date.get_group('2024-03-25').sort_values('mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicate if you are sussed out of having to redo this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.backup(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_stitch = []\n",
    "mice_to_stitch = []\n",
    "for date, data in sessions_by_date:\n",
    "    for mouse in mouse_list:\n",
    "        mouse_by_date = data.loc[data['mouse'] == mouse]\n",
    "        if len(mouse_by_date) > 1:\n",
    "            days_to_stitch.append(date)\n",
    "            mice_to_stitch.append(mouse)\n",
    "            print(f\"on {date}, {mouse} has {len(mouse_by_date)} sessions\")\n",
    "if not days_to_stitch:\n",
    "    print(\"no sessions to stitch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch sessions from the same mouse on the same day\n",
    "def stitch_sessions(session_1, session_2):\n",
    "    session_1_basics = helper.get_session_basics(session_1)\n",
    "    time_offset = session_1_basics['session_time']\n",
    "    block_offset = session_1_basics['num_blocks']\n",
    "    trial_offset = session_1_basics['num_trials']\n",
    "    \n",
    "    session_2.session_time = session_2.session_time + time_offset\n",
    "    session_2.block_num = session_2.block_num + block_offset\n",
    "    session_2.session_trial_num= session_2.session_trial_num + trial_offset\n",
    "\n",
    "    stitched_session = pd.concat([session_1, session_2])\n",
    "    return stitched_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it if session stitching is needed, nothing would happen otherwise\n",
    "# has to run more than once if there are more than 2 sessions. fix it for the next round pls\n",
    "if not days_to_stitch:\n",
    "    print(\"no sessions to stitch!\")\n",
    "else:\n",
    "    for d, m in zip(days_to_stitch, mice_to_stitch):\n",
    "        day = sessions_by_date.get_group(d)\n",
    "        sessions_to_stitch = day[day['mouse'] == m]\n",
    "\n",
    "        session_1_dir = utils.generate_events_processed_path(data_folder, sessions_to_stitch.iloc[0])\n",
    "        session_2_dir = utils.generate_events_processed_path(data_folder, sessions_to_stitch.iloc[1])\n",
    "\n",
    "        if os.path.exists(session_1_dir) and os.path.exists(session_2_dir):\n",
    "            session_1 = pd.read_csv(session_1_dir)\n",
    "            session_2 = pd.read_csv(session_2_dir)\n",
    "            stitched_session = stitch_sessions(session_1, session_2) \n",
    "            #should change to stitch events. stitch sessions should be deleted. to follow nomanclature, all session should be renamed to events!!\n",
    "\n",
    "            stitched_session.to_csv(session_1_dir, index=False)\n",
    "            shutil.rmtree(os.path.join(data_folder, sessions_to_stitch.iloc[1].dir))\n",
    "            print(f\"{d} {m} session 2 deleted\")\n",
    "        else:\n",
    "            print(\"one of the sessions do not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_all, sessions_training = generate_session_logs(data_folder)\n",
    "\n",
    "sessions_training.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize sessions log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sessions_training(data_folder, save_log=True):\n",
    "    _, sessions_training = generate_session_logs(data_folder, save_logs=False)\n",
    "    session_info_list = []\n",
    "    for _, session_info in sessions_training.iterrows():\n",
    "        events_processed = pd.read_csv(utils.generate_events_processed_path(data_folder, session_info), low_memory=False)\n",
    "        session_basics = helper.get_session_basics(events_processed)\n",
    "        session_basics['dir'] = session_info['dir']\n",
    "        session_info_list.append(session_basics)\n",
    "    sessions_info = pd.DataFrame(session_info_list)\n",
    "    corrected_sessions_training = pd.merge(sessions_training, sessions_info, on=\"dir\")\n",
    "    corrected_sessions_training = corrected_sessions_training.drop(columns=['index', 'total_reward', 'total_trial', 'total_reward'])\n",
    "    corrected_sessions_training = corrected_sessions_training.groupby('mouse', group_keys=False).apply(helper.assign_session_numbers)\n",
    "    corrected_sessions_training['cohort'] = cohort\n",
    "    if save_log:\n",
    "        utils.save_as_csv(df=corrected_sessions_training, folder=data_folder, filename=f'sessions_training_{exp}_{cohort}_pre.csv')\n",
    "    return corrected_sessions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = correct_sessions_training(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = utils.load_data(os.path.join(data_folder, f'sessions_training_{exp}_{cohort}_pre.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trials(session_info, events):\n",
    "    trial_info_list = []\n",
    "    for t in range(int(session_info.num_trials)):\n",
    "        trial = events.loc[events['session_trial_num'] == t]\n",
    "        trial_basics = helper.get_trial_basics(trial)\n",
    "        trial_info_list.append(trial_basics)\n",
    "    trials = pd.DataFrame(trial_info_list)\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all trials based on events processed\n",
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try: \n",
    "        trials_path = utils.generate_trials_path(data_folder, session_info)\n",
    "        # if os.path.isfile(trials_path):\n",
    "        #     continue\n",
    "        \n",
    "        events_processed = pd.read_csv(utils.generate_events_processed_path(data_folder, session_info))\n",
    "        trials = generate_trials(session_info, events_processed)\n",
    "\n",
    "        trials.to_csv(trials_path)\n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "\n",
    "if len(problematic_sessions) > 0:\n",
    "    display(problematic_sessions)\n",
    "else:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        trials_analyzed_path = utils.generate_trials_analyzed_path(data_folder, session_info)\n",
    "        # if os.path.isfile(trials_analyzed_path):\n",
    "        #     continue\n",
    "        \n",
    "        session_by_trial = utils.load_data(utils.generate_events_processed_path(data_folder, session_info)).groupby('session_trial_num')\n",
    "        trials = utils.load_data(utils.generate_trials_path(data_folder, session_info))\n",
    "        trials_data = helper.get_trial_data_df(session_by_trial)\n",
    "        trials_analyzed = pd.merge(trials, trials_data, on='session_trial_num')\n",
    "        trials_analyzed['group'] = session_info.group #assigning trial type manually\n",
    "        trials_analyzed.to_csv(trials_analyzed_path)\n",
    "    except:\n",
    "        display(session_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUCKYCHARM",
   "language": "python",
   "name": "luckycharm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
