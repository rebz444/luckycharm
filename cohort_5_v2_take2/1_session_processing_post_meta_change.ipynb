{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import session_processing_helper_c5v2 as helper\n",
    "import utils_c5v2 as utils\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rebekahzhang/data/behavior_data/20250113-20250321/exp2/cohort_7\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/rebekahzhang/data/behavior_data'\n",
    "period = '20240502-20241219'\n",
    "exp = \"exp2\"\n",
    "cohort = \"cohort_7\"\n",
    "data_folder = os.path.join(data_dir, period, exp, cohort)\n",
    "print(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check session folders have both meta and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_events, missing_meta, empty_meta, empty_events = helper.check_session_files(data_folder)\n",
    "if not (missing_meta.empty and missing_events.empty and empty_meta.empty and empty_events.empty):\n",
    "    print(\"\\nFile check results:\")\n",
    "    if not missing_meta.empty:\n",
    "        print(\"\\nSessions missing meta files:\")\n",
    "        display(missing_meta)\n",
    "    if not missing_events.empty:\n",
    "        print(\"\\nSessions missing events files:\")\n",
    "        display(missing_events)\n",
    "    if not empty_meta.empty:\n",
    "        print(\"\\nSessions with empty meta files:\")\n",
    "        display(empty_meta)\n",
    "    if not empty_events.empty:\n",
    "        print(\"\\nSessions with empty events files:\")\n",
    "        display(empty_events)\n",
    "else:\n",
    "    print(\"\\nAll sessions have non-empty meta and events files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.delete_folders(missing_meta.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.delete_folders(missing_events.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.delete_folders(empty_meta.dir.tolist(), data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.delete_folders(empty_events.dir.tolist(), data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save sessions log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate session log using meta data from each session and add columns of basic info to each session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-run after every quality control steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_all, sessions_training = helper.generate_session_logs(data_folder)\n",
    "sessions_training.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted sessions\n",
    "doesn't need to run when data folder is cleaned \n",
    "<br>\n",
    "sessions_all needs to be regenerated after every cleaning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove test sessions\n",
    "currently not needed cuz test sessions are not sorted into exp folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_test = sessions_all.loc[sessions_all.mouse=='test']\n",
    "utils.remove_sessions(sessions_test, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove crashed sessions\n",
    "remove the sessions with ending_code==nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_ended_nan = sessions_all[sessions_all.ending_code.isna()]\n",
    "utils.remove_sessions(sessions_ended_nan, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reomve sessions that crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_crashed = pd.DataFrame(columns=sessions_training.columns)\n",
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        events_path = utils.generate_events_path(data_folder, session_info)\n",
    "        events = pd.read_csv(events_path, low_memory=False)\n",
    "        session_end = events.loc[(events.key=='session') & (events.value==0)]\n",
    "\n",
    "        if not len(session_end)==1:\n",
    "            sessions_crashed = pd.concat([sessions_crashed, session_info.to_frame().T], ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "\n",
    "if len(problematic_sessions) > 0:\n",
    "    print(\"cannot open: \")\n",
    "    display(problematic_sessions)\n",
    "\n",
    "if len(sessions_crashed) > 0:\n",
    "    print(\"crashed sessions: \")\n",
    "    display(sessions_crashed)\n",
    "else:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.remove_sessions(sessions_crashed, data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for short sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_threshold = 20\n",
    "sessions_short = sessions_all[(sessions_all['total_trial'] < short_threshold) | sessions_all['total_trial'].isna()]\n",
    "if len(sessions_short)>0:\n",
    "    display(sessions_short)\n",
    "else: \n",
    "    print('no short sessions to be checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove short sessions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.remove_sessions(sessions_short, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load session log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = utils.load_data(os.path.join(data_folder, 'sessions_training.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        events_processed_path = utils.generate_events_processed_path(data_folder, session_info)\n",
    "        if os.path.isfile(events_processed_path):\n",
    "            continue\n",
    "        events = pd.read_csv(utils.generate_events_path(data_folder, session_info), low_memory=False)\n",
    "        events = helper.process_events(session_info, events)\n",
    "        events_processed = events.groupby('session_trial_num', group_keys=False).apply(helper.add_trial_time)\n",
    "        events_processed.to_csv(events_processed_path)\n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "\n",
    "if len(problematic_sessions) > 0:\n",
    "    display(problematic_sessions)\n",
    "else:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set curation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated rounds dict cuz we no longer run more than 1 session per day\n",
    "# mouse_list = utils.generate_mouse_list(sessions_all)\n",
    "# mouse_list = ['RZ047','RZ049','RZ050','RZ051','RZ052','RZ053','RZ054','RZ055','RZ056']\n",
    "# mouse_list = [\"RZ057\", \"RZ058\", \"RZ059\", \"RZ061\", \"RZ062\", \"RZ063\", \"RZ064\",\n",
    "#                      \"RZ065\", \"RZ067\", \"RZ068\", \"RZ069\", \"RZ070\", \"RZ072\"]\n",
    "mouse_list = [\"RZ057\", \"RZ058\", \"RZ059\", \"RZ061\", \"RZ062\", \"RZ063\",\n",
    "                     \"RZ065\", \"RZ067\", \"RZ068\", \"RZ069\", \"RZ070\"]\n",
    "print(mouse_list)\n",
    "sessions_by_date = sessions_training.groupby('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing sessions\n",
    "this is not the proper way to deal with this. should have it populated with mean and variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_missing_sessions = True\n",
    "for date, data in sessions_by_date:\n",
    "    for mouse in mouse_list:\n",
    "        mouse_by_date = data.loc[data['mouse'] == mouse]\n",
    "        if len(mouse_by_date) < 1:\n",
    "            no_missing_sessions = False\n",
    "            print(f\"on {date}, {mouse} has missing sessions\")\n",
    "if no_missing_sessions:\n",
    "    print(\"no missing sessions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_by_date.get_group('2025-02-11').sort_values('mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### duplicate if you are sussed out of having to redo this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.backup(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_stitch = []\n",
    "mice_to_stitch = []\n",
    "for date, data in sessions_by_date:\n",
    "    for mouse in mouse_list:\n",
    "        mouse_by_date = data.loc[data['mouse'] == mouse]\n",
    "        if len(mouse_by_date) > 1:\n",
    "            days_to_stitch.append(date)\n",
    "            mice_to_stitch.append(mouse)\n",
    "            print(f\"on {date}, {mouse} has {len(mouse_by_date)} sessions\")\n",
    "if not days_to_stitch:\n",
    "    print(\"no sessions to stitch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it if session stitching is needed, nothing would happen otherwise\n",
    "# has to run more than once if there are more than 2 sessions. fix it for the next round pls\n",
    "if not days_to_stitch:\n",
    "    print(\"no sessions to stitch!\")\n",
    "else:\n",
    "    for d, m in zip(days_to_stitch, mice_to_stitch):\n",
    "        day = sessions_by_date.get_group(d)\n",
    "        sessions_to_stitch = day[day['mouse'] == m]\n",
    "\n",
    "        session_1_dir = utils.generate_events_processed_path(data_folder, sessions_to_stitch.iloc[0])\n",
    "        session_2_dir = utils.generate_events_processed_path(data_folder, sessions_to_stitch.iloc[1])\n",
    "\n",
    "        if os.path.exists(session_1_dir) and os.path.exists(session_2_dir):\n",
    "            session_1 = pd.read_csv(session_1_dir)\n",
    "            session_2 = pd.read_csv(session_2_dir)\n",
    "            stitched_session = helper.stitch_sessions(session_1, session_2) \n",
    "            #should change to stitch events. stitch sessions should be deleted. to follow nomanclature, all session should be renamed to events!!\n",
    "\n",
    "            stitched_session.to_csv(session_1_dir, index=False)\n",
    "            shutil.rmtree(os.path.join(data_folder, sessions_to_stitch.iloc[1].dir))\n",
    "            print(f\"{d} {m} session 2 deleted\")\n",
    "        else:\n",
    "            print(\"one of the sessions do not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_all, sessions_training = helper.generate_session_logs(data_folder)\n",
    "\n",
    "sessions_training.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize sessions log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_sessions_training(data_folder, save_log=True):\n",
    "    _, sessions_training = helper.generate_session_logs(data_folder, save_logs=False)\n",
    "    session_info_list = []\n",
    "    for _, session_info in sessions_training.iterrows():\n",
    "        events_processed = pd.read_csv(utils.generate_events_processed_path(data_folder, session_info), low_memory=False)\n",
    "        session_basics = helper.get_session_basics(events_processed)\n",
    "        session_basics['dir'] = session_info['dir']\n",
    "        session_info_list.append(session_basics)\n",
    "    sessions_info = pd.DataFrame(session_info_list)\n",
    "    corrected_sessions_training = pd.merge(sessions_training, sessions_info, on=\"dir\")\n",
    "    corrected_sessions_training = corrected_sessions_training.drop(columns=['index', 'total_reward', 'total_trial', 'total_reward'])\n",
    "    corrected_sessions_training = corrected_sessions_training.groupby('mouse', group_keys=False).apply(helper.assign_session_numbers)\n",
    "    if save_log:\n",
    "        utils.save_as_csv(df=corrected_sessions_training, folder=data_folder, filename='sessions_training.csv')\n",
    "    return corrected_sessions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = correct_sessions_training(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_training = utils.load_data(os.path.join(data_folder, 'sessions_training.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all trials based on events processed\n",
    "problematic_sessions = pd.DataFrame(columns=sessions_training.columns)\n",
    "for _, session_info in sessions_training.iterrows():\n",
    "    try: \n",
    "        trials_path = utils.generate_trials_path(data_folder, session_info)\n",
    "        if os.path.isfile(trials_path):\n",
    "            continue\n",
    "        \n",
    "        events_processed = pd.read_csv(utils.generate_events_processed_path(data_folder, session_info))\n",
    "        trials = helper.generate_trials(session_info, events_processed)\n",
    "\n",
    "        trials.to_csv(trials_path)\n",
    "    except:\n",
    "        problematic_sessions = pd.concat([problematic_sessions, session_info.to_frame().T], ignore_index=True)\n",
    "\n",
    "if len(problematic_sessions) > 0:\n",
    "    display(problematic_sessions)\n",
    "else:\n",
    "    print(\"all sessions are perfect! woohoo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, session_info in sessions_training.iterrows():\n",
    "    try:\n",
    "        trials_analyzed_path = utils.generate_trials_analyzed_path(data_folder, session_info)\n",
    "        if os.path.isfile(trials_analyzed_path):\n",
    "            continue\n",
    "        \n",
    "        session_by_trial = utils.load_data(utils.generate_events_processed_path(data_folder, session_info)).groupby('session_trial_num')\n",
    "        trials = utils.load_data(utils.generate_trials_path(data_folder, session_info))\n",
    "        trials_data = helper.get_trial_data_df(session_by_trial)\n",
    "        trials_analyzed = pd.merge(trials, trials_data, on='session_trial_num')\n",
    "        trials_analyzed['group'] = session_info.group #assigning trial type manually\n",
    "        trials_analyzed.to_csv(trials_analyzed_path)\n",
    "    except:\n",
    "        display(session_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debug debug debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/Users/rebekahzhang/data/behavior_data'\n",
    "# exp = \"exp2\"\n",
    "# cohort = \"c567\"\n",
    "# data_folder = os.path.join(data_dir, exp, cohort)\n",
    "# sessions_training = pd.read_csv(os.path.join(data_folder, 'sessions_training.csv'), index_col=0)\n",
    "\n",
    "# sessions_training_post_meta_change = sessions_training.loc[sessions_training.date < \"2024-05-03\"]\n",
    "\n",
    "# for _, session_info in sessions_training_post_meta_change.iterrows():\n",
    "#     try:\n",
    "#         trials_analyzed_path = utils.generate_trials_analyzed_path(data_folder, session_info)\n",
    "#         # if os.path.isfile(trials_analyzed_path):\n",
    "#         #     continue\n",
    "        \n",
    "#         session_by_trial = utils.load_data(utils.generate_events_processed_path(data_folder, session_info)).groupby('session_trial_num')\n",
    "#         trials = utils.load_data(utils.generate_trials_path(data_folder, session_info))\n",
    "#         trials_data = helper.get_trial_data_df(session_by_trial)\n",
    "#         trials_analyzed = pd.merge(trials, trials_data, on='session_trial_num')\n",
    "#         trials_analyzed['group'] = session_info.group #assigning trial type manually\n",
    "#         trials_analyzed.to_csv(trials_analyzed_path)\n",
    "#     except:\n",
    "#         display(session_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LUCKYCHARM",
   "language": "python",
   "name": "luckycharm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
